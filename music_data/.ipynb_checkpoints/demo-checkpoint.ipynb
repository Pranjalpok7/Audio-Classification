{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde63ba3-08c3-4d6b-93b6-3cd338fa2ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.nn import knn_graph\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21acdb25-d5d0-4cee-bc42-c37a56986356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioGraphDataset(Dataset):\n",
    "    def __init__(self, root, path, k_neigh=5, enable=False, transform=None, pre_transform=None, pre_filter=None):\n",
    "        \"\"\"\n",
    "        Initialize the Audio Graph Dataset\n",
    "        Args:\n",
    "            root: Root directory for processed files\n",
    "            path: List containing paths to [csv_file, hdf5_file]\n",
    "            k_neigh: Number of nearest neighbors for graph construction\n",
    "            enable: Flag for extended processing mode\n",
    "            transform: Transform to be applied to the data\n",
    "            pre_transform: Transform to be applied before processing\n",
    "            pre_filter: Filter to be applied before processing\n",
    "        \"\"\"\n",
    "        self.csv_path = path[0]\n",
    "        self.hdf5_path = path[1]\n",
    "        self.enable = enable\n",
    "        self.k_neigh = k_neigh\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # Return the actual filenames without full paths\n",
    "        return [osp.basename(self.csv_path), osp.basename(self.hdf5_path)]\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        # Return the directory containing the raw files\n",
    "        return osp.dirname(self.csv_path)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # Return list of processed file names based on enable flag\n",
    "        if not self.enable:\n",
    "            return [f'data_{i}.pt' for i in range(10)]\n",
    "        else:\n",
    "            return [f'data_{i}.pt' for i in range(109)]\n",
    "\n",
    "    def download(self):\n",
    "        # No download needed as we're using local files\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Process the raw data into graph format\n",
    "        - Loads features and labels from HDF5\n",
    "        - Performs cross-validation splits\n",
    "        - Creates graph structure using kNN\n",
    "        - Saves processed graphs\n",
    "        \"\"\"\n",
    "        # Load metadata and features using full paths\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        self.data = h5py.File(self.hdf5_path, 'r')\n",
    "        \n",
    "        # Convert features and labels to torch tensors\n",
    "        X_tensor = torch.tensor(np.array(self.data['features']))\n",
    "        Y_tensor = torch.tensor(np.array(self.data['labels']))\n",
    "\n",
    "        if not self.enable:\n",
    "            # Standard processing mode - 10 fold cross validation\n",
    "            self._process_folds(X_tensor, Y_tensor, 0, 10)\n",
    "        else:\n",
    "            # Extended processing mode - varying k neighbors\n",
    "            for k in range(5, 115, 10):\n",
    "                start_idx = (k - 5) // 10 * 10\n",
    "                self._process_folds(X_tensor, Y_tensor, start_idx, 10, k)\n",
    "\n",
    "    def _process_folds(self, X_tensor, Y_tensor, start_idx, num_folds, k_neighbors=None):\n",
    "        \"\"\"\n",
    "        Process data for multiple cross-validation folds\n",
    "        \"\"\"\n",
    "        test_idx = 1\n",
    "        val_idx = 2\n",
    "        k = k_neighbors if k_neighbors is not None else self.k_neigh\n",
    "\n",
    "        for j in range(start_idx, start_idx + num_folds):\n",
    "            # Get train/test/validation splits\n",
    "            idx_train, idx_test, idx_val = self._cross_validation_split(\n",
    "                self.df, test_idx, val_idx)\n",
    "            \n",
    "            # Normalize features\n",
    "            X_tensor_norm = self._normalize_features(X_tensor, idx_train)\n",
    "            \n",
    "            # Create graph data object\n",
    "            data = self._create_graph(X_tensor_norm, Y_tensor, \n",
    "                                    idx_train, idx_test, idx_val, k)\n",
    "            \n",
    "            # Save processed data\n",
    "            torch.save(data, osp.join(self.processed_dir, f'data_{j}.pt'))\n",
    "            \n",
    "            # Update indices for next fold\n",
    "            test_idx = (test_idx + 1) % 11\n",
    "            val_idx = (val_idx + 1) % 11\n",
    "            if val_idx == 0:\n",
    "                val_idx = 1\n",
    "\n",
    "    def _normalize_features(self, X_tensor, idx_train):\n",
    "        \"\"\"\n",
    "        Normalize features using min-max scaling based on training set\n",
    "        \"\"\"\n",
    "        mini = X_tensor[idx_train].min()\n",
    "        maxi = X_tensor[idx_train].max()\n",
    "        return (X_tensor - mini) / (maxi - mini)\n",
    "\n",
    "    def _cross_validation_split(self, df, idx_test, idx_val):\n",
    "        \"\"\"\n",
    "        Create train/test/validation splits using fold information\n",
    "        \"\"\"\n",
    "        df = df.reset_index(drop=True)\n",
    "        df_train = df.copy()\n",
    "        \n",
    "        # Get test indices and remove from training\n",
    "        idx_test_mask = df.index[df.fold == idx_test]\n",
    "        df_train.drop(idx_test_mask, inplace=True)\n",
    "        \n",
    "        # Get validation indices and remove from training\n",
    "        idx_val_mask = df.index[df.fold == idx_val]\n",
    "        df_train.drop(idx_val_mask, inplace=True)\n",
    "        \n",
    "        # Remaining indices are training set\n",
    "        idx_train_mask = df_train.index\n",
    "        \n",
    "        return idx_train_mask, idx_test_mask, idx_val_mask\n",
    "\n",
    "    def _create_graph(self, X_tensor, Y_tensor, idx_train, idx_test, idx_val, k):\n",
    "        \"\"\"\n",
    "        Create a graph using feature tensors and kNN\n",
    "        \"\"\"\n",
    "        # Convert indices to tensors\n",
    "        train_mask = torch.tensor(idx_train)\n",
    "        test_mask = torch.tensor(idx_test)\n",
    "        val_mask = torch.tensor(idx_val)\n",
    "\n",
    "        # Create edge connections using kNN\n",
    "        edge_index = knn_graph(X_tensor, k=k, batch=Y_tensor, loop=False)\n",
    "\n",
    "        # Create and return the graph data object\n",
    "        return Data(\n",
    "            x=X_tensor,\n",
    "            edge_index=edge_index,\n",
    "            y=Y_tensor,\n",
    "            train_mask=train_mask,\n",
    "            test_mask=test_mask,\n",
    "            val_mask=val_mask\n",
    "        )\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"Load and return a specific graph\"\"\"\n",
    "        return torch.load(osp.join(self.processed_dir, f'data_{idx}.pt')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b399a5-8975-4cdf-86b5-105372d98415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "dataset = AudioGraphDataset(\n",
    "    root='processed_data',\n",
    "    path=['nepali_music_metadata.csv', 'nepali_features.hdf5'],\n",
    "    k_neigh=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550417b-c1ac-403d-ad5d-d046602fcd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a specific graph\n",
    "graph = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4bab1d-ff34-476b-b463-6232b101c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNAudioClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes, dropout=0.3):\n",
    "        \"\"\"\n",
    "        GCN model for audio classification\n",
    "        Args:\n",
    "            num_features: Number of input features (1024 for YaMNet)\n",
    "            hidden_channels: Number of hidden units\n",
    "            num_classes: Number of output classes\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, num_classes)\n",
    "        self.dropout = dropout\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First Graph Convolution\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)  # Apply batch normalization\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Second Graph Convolution\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)  # Apply batch normalization\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Final Graph Convolution\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964a66f-3256-496e-96d9-83b14f4f0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, optimizer, epochs=200):\n",
    "    \"\"\"\n",
    "    Train the GCN model with early stopping\n",
    "    Args:\n",
    "        model: GCN model instance\n",
    "        data: PyG Data object containing the graph\n",
    "        optimizer: PyTorch optimizer\n",
    "        epochs: Number of training epochs\n",
    "    Returns:\n",
    "        lists of training/validation losses and accuracies\n",
    "    \"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 20  # Number of epochs to wait for improvement\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        \n",
    "        # Calculate loss only on training nodes\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x, data.edge_index)\n",
    "            \n",
    "            # Training metrics\n",
    "            train_loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask]).item()\n",
    "            train_acc = accuracy_score(data.y[data.train_mask].cpu(), \n",
    "                                    out[data.train_mask].argmax(dim=1).cpu())\n",
    "            \n",
    "            # Validation metrics\n",
    "            val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask]).item()\n",
    "            val_acc = accuracy_score(data.y[data.val_mask].cpu(), \n",
    "                                   out[data.val_mask].argmax(dim=1).cpu())\n",
    "            \n",
    "        # Append metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1:03d}, '\n",
    "                  f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b746baa6-a252-41fc-b2ad-2fb904cab424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test set\n",
    "    Args:\n",
    "        model: Trained GCN model\n",
    "        data: PyG Data object containing the graph\n",
    "    Returns:\n",
    "        dict containing various metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out[data.test_mask].argmax(dim=1).cpu()\n",
    "        true = data.y[data.test_mask].cpu()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(true, pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(true, pred, average='weighted')\n",
    "        \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9ca01-3c41-45ad-8488-09ac6515d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train_k_fold(dataset, hidden_channels=32, dropout=0.3, epochs=200, lr=0.01):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross validation training with L2 regularization and learning rate scheduling\n",
    "    Args:\n",
    "        dataset: AudioGraphDataset instance\n",
    "        hidden_channels: Number of hidden units in GCN\n",
    "        dropout: Dropout rate\n",
    "        epochs: Number of epochs per fold\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Store metrics for each fold\n",
    "    fold_metrics = defaultdict(list)\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    all_train_accs = []\n",
    "    all_val_accs = []\n",
    "    \n",
    "    print(f\"Training on {len(dataset)} folds...\")\n",
    "    \n",
    "    for fold in range(len(dataset)):\n",
    "        print(f\"\\nTraining Fold {fold + 1}/{len(dataset)}\")\n",
    "        \n",
    "        # Get graph data for current fold\n",
    "        data = dataset[fold].to(device)\n",
    "        \n",
    "        # # Initialize model with YaMNet dimensionality\n",
    "        model = GCNAudioClassifier(\n",
    "            num_features=1024,  # Changed from 128 to 1024\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_classes=10,    \n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)  # L2 regularization\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)  # LR scheduling\n",
    "        \n",
    "        # Train on this fold\n",
    "        train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "            model=model,\n",
    "            data=data,\n",
    "            optimizer=optimizer,\n",
    "            epochs=epochs\n",
    "        )\n",
    "        \n",
    "        # Store training curves\n",
    "        all_train_losses.append(train_losses)\n",
    "        all_val_losses.append(val_losses)\n",
    "        all_train_accs.append(train_accs)\n",
    "        all_val_accs.append(val_accs)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_metrics = evaluate_model(model, data)\n",
    "        for metric, value in test_metrics.items():\n",
    "            fold_metrics[metric].append(value)\n",
    "        \n",
    "        print(f\"Fold {fold + 1} Test Metrics:\")\n",
    "        for metric, value in test_metrics.items():\n",
    "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "    \n",
    "    # Calculate average metrics across folds\n",
    "    print(\"\\nOverall Cross-Validation Results:\")\n",
    "    for metric, values in fold_metrics.items():\n",
    "        mean_value = np.mean(values)\n",
    "        std_value = np.std(values)\n",
    "        print(f\"{metric.capitalize()}: {mean_value:.4f} Â± {std_value:.4f}\")\n",
    "    \n",
    "    return fold_metrics, (all_train_losses, all_val_losses, all_train_accs, all_val_accs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfedb77-4e75-4c60-8dab-632a1b954c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_fold_curves(all_train_losses, all_val_losses, all_train_accs, all_val_accs):\n",
    "    \"\"\"Plot average training curves across all folds with standard deviation\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Find the maximum length across all folds\n",
    "    max_length = max(len(fold) for fold in all_train_losses)\n",
    "    \n",
    "    # Pad shorter folds with NaN values to make all folds the same length\n",
    "    def pad_fold(fold, max_length):\n",
    "        return np.pad(fold, (0, max_length - len(fold)), mode='constant', constant_values=np.nan)\n",
    "    \n",
    "    all_train_losses = [pad_fold(fold, max_length) for fold in all_train_losses]\n",
    "    all_val_losses = [pad_fold(fold, max_length) for fold in all_val_losses]\n",
    "    all_train_accs = [pad_fold(fold, max_length) for fold in all_train_accs]\n",
    "    all_val_accs = [pad_fold(fold, max_length) for fold in all_val_accs]\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_train_losses = np.array(all_train_losses)\n",
    "    all_val_losses = np.array(all_val_losses)\n",
    "    all_train_accs = np.array(all_train_accs)\n",
    "    all_val_accs = np.array(all_val_accs)\n",
    "    \n",
    "    # Calculate mean and std for losses\n",
    "    train_losses_mean = np.nanmean(all_train_losses, axis=0)  # Ignore NaNs\n",
    "    train_losses_std = np.nanstd(all_train_losses, axis=0)    # Ignore NaNs\n",
    "    val_losses_mean = np.nanmean(all_val_losses, axis=0)      # Ignore NaNs\n",
    "    val_losses_std = np.nanstd(all_val_losses, axis=0)        # Ignore NaNs\n",
    "    \n",
    "    # Calculate mean and std for accuracies\n",
    "    train_accs_mean = np.nanmean(all_train_accs, axis=0)      # Ignore NaNs\n",
    "    train_accs_std = np.nanstd(all_train_accs, axis=0)        # Ignore NaNs\n",
    "    val_accs_mean = np.nanmean(all_val_accs, axis=0)          # Ignore NaNs\n",
    "    val_accs_std = np.nanstd(all_val_accs, axis=0)            # Ignore NaNs\n",
    "    \n",
    "    # Plot losses\n",
    "    epochs = range(1, len(train_losses_mean) + 1)\n",
    "    ax1.plot(epochs, train_losses_mean, label='Train Loss')\n",
    "    ax1.fill_between(epochs, train_losses_mean - train_losses_std, \n",
    "                    train_losses_mean + train_losses_std, alpha=0.2)\n",
    "    ax1.plot(epochs, val_losses_mean, label='Validation Loss')\n",
    "    ax1.fill_between(epochs, val_losses_mean - val_losses_std,\n",
    "                    val_losses_mean + val_losses_std, alpha=0.2)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Average Training and Validation Loss')\n",
    "    \n",
    "    # Plot accuracies\n",
    "    ax2.plot(epochs, train_accs_mean, label='Train Accuracy')\n",
    "    ax2.fill_between(epochs, train_accs_mean - train_accs_std,\n",
    "                    train_accs_mean + train_accs_std, alpha=0.2)\n",
    "    ax2.plot(epochs, val_accs_mean, label='Validation Accuracy')\n",
    "    ax2.fill_between(epochs, val_accs_mean - val_accs_std,\n",
    "                    val_accs_mean + val_accs_std, alpha=0.2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.set_title('Average Training and Validation Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c77e77-d49b-4407-b03b-d35ac3987358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your existing dataset\n",
    "fold_metrics, curves = train_k_fold(\n",
    "    dataset=dataset,\n",
    "    hidden_channels=32,  # Reduced hidden channels\n",
    "    dropout=0.3,         # Adjusted dropout\n",
    "    epochs=200,\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# Unpack curves\n",
    "all_train_losses, all_val_losses, all_train_accs, all_val_accs = curves\n",
    "\n",
    "# Plot average learning curves across folds\n",
    "plot_k_fold_curves(all_train_losses, all_val_losses, all_train_accs, all_val_accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
