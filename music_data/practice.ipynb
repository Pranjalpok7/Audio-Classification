{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ce2f30-b36e-4f6f-a1cb-02e3c269ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.nn import knn_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9127ac5-4941-4ddd-b17d-aae43f123abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioGraphDataset(Dataset):\n",
    "    def __init__(self, root, path, k_neigh=5, split_ratio=(0.7, 0.15), transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the Audio Graph Dataset\n",
    "        Args:\n",
    "            root: Root directory for processed files\n",
    "            path: List containing paths to [csv_file, hdf5_file]\n",
    "            k_neigh: Number of nearest neighbors for graph construction\n",
    "            split_ratio: Tuple of (train_ratio, val_ratio)\n",
    "        \"\"\"\n",
    "        self.csv_path = path[0]\n",
    "        self.hdf5_path = path[1]\n",
    "        self.k_neigh = k_neigh\n",
    "        self.split_ratio = split_ratio\n",
    "        super().__init__(root, transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [osp.basename(self.csv_path), osp.basename(self.hdf5_path)]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Load data\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        with h5py.File(self.hdf5_path, 'r') as hf:\n",
    "            features = torch.tensor(hf['features'][:], dtype=torch.float32)\n",
    "            labels = torch.tensor(hf['labels'][:], dtype=torch.long)\n",
    "\n",
    "        # Create single split\n",
    "        num_nodes = features.size(0)\n",
    "        indices = torch.randperm(num_nodes)\n",
    "        train_end = int(num_nodes * self.split_ratio[0])\n",
    "        val_end = train_end + int(num_nodes * self.split_ratio[1])\n",
    "\n",
    "        # Create masks\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        \n",
    "        train_mask[indices[:train_end]] = True\n",
    "        val_mask[indices[train_end:val_end]] = True\n",
    "        test_mask[indices[val_end:]] = True\n",
    "\n",
    "        # Normalize features using training data\n",
    "        mean = features[train_mask].mean(dim=0)\n",
    "        std = features[train_mask].std(dim=0)\n",
    "        features = (features - mean) / std\n",
    "\n",
    "        # Create graph structure\n",
    "        edge_index = knn_graph(features, k=self.k_neigh, loop=False)\n",
    "\n",
    "        # Create single data object\n",
    "        data = Data(\n",
    "            x=features,\n",
    "            edge_index=edge_index,\n",
    "            y=labels,\n",
    "            train_mask=train_mask,\n",
    "            val_mask=val_mask,\n",
    "            test_mask=test_mask\n",
    "        )\n",
    "\n",
    "        torch.save(data, osp.join(self.processed_dir, 'data.pt'))\n",
    "\n",
    "    def len(self):\n",
    "        return 1  # Single graph dataset\n",
    "\n",
    "    def get(self, idx):\n",
    "        return torch.load(osp.join(self.processed_dir, 'data.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96826b78-49e5-4097-858a-7547b4b44a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, 64)\n",
    "        self.conv2 = GCNConv(64, 32)\n",
    "        self.conv3 = GCNConv(32, num_classes)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5212a52-92d7-4f27-b7c3-94d291fb04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        \n",
    "        acc_train = (pred[data.train_mask] == data.y[data.train_mask]).float().mean()\n",
    "        acc_val = (pred[data.val_mask] == data.y[data.val_mask]).float().mean()\n",
    "        acc_test = (pred[data.test_mask] == data.y[data.test_mask]).float().mean()\n",
    "    \n",
    "    return acc_train.item(), acc_val.item(), acc_test.item()\n",
    "\n",
    "def run_training(root, path, num_epochs=200):\n",
    "    # Load dataset\n",
    "    dataset = AudioGraphDataset(root=root, path=path)\n",
    "    data = dataset[0].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize model\n",
    "    model = GCN(num_features=data.num_features, \n",
    "               num_classes=data.y.unique().size(0)).to(data.x.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        loss = train(model, data, optimizer)\n",
    "        train_acc, val_acc, test_acc = evaluate(model, data)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "    \n",
    "    print('\\nFinal Results:')\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    train_acc, val_acc, test_acc = evaluate(model, data)\n",
    "    print(f'Best Model - Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667b5f99-be21-495a-b9bc-1af5e18074ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "/tmp/ipykernel_13925/3028526823.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(osp.join(self.processed_dir, 'data.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 2.5913, Train: 0.2348, Val: 0.2290, Test: 0.2651\n",
      "Epoch: 002, Loss: 4.9368, Train: 0.3756, Val: 0.3610, Test: 0.3488\n",
      "Epoch: 003, Loss: 3.9707, Train: 0.5452, Val: 0.5170, Test: 0.5137\n",
      "Epoch: 004, Loss: 3.0368, Train: 0.5474, Val: 0.5328, Test: 0.5468\n",
      "Epoch: 005, Loss: 3.4758, Train: 0.5940, Val: 0.5859, Test: 0.5915\n",
      "Epoch: 006, Loss: 3.1075, Train: 0.6196, Val: 0.6216, Test: 0.6098\n",
      "Epoch: 007, Loss: 2.5600, Train: 0.6240, Val: 0.6166, Test: 0.6056\n",
      "Epoch: 008, Loss: 2.2971, Train: 0.6432, Val: 0.6349, Test: 0.6123\n",
      "Epoch: 009, Loss: 2.1712, Train: 0.6480, Val: 0.6307, Test: 0.6305\n",
      "Epoch: 010, Loss: 2.0167, Train: 0.6528, Val: 0.6448, Test: 0.6413\n",
      "Epoch: 011, Loss: 1.9106, Train: 0.6752, Val: 0.6656, Test: 0.6587\n",
      "Epoch: 012, Loss: 1.6155, Train: 0.6750, Val: 0.6747, Test: 0.6678\n",
      "Epoch: 013, Loss: 1.5939, Train: 0.6777, Val: 0.6739, Test: 0.6578\n",
      "Epoch: 014, Loss: 1.4183, Train: 0.6921, Val: 0.6896, Test: 0.6686\n",
      "Epoch: 015, Loss: 1.2795, Train: 0.6910, Val: 0.6830, Test: 0.6611\n",
      "Epoch: 016, Loss: 1.2582, Train: 0.6864, Val: 0.6763, Test: 0.6578\n",
      "Epoch: 017, Loss: 1.2612, Train: 0.7006, Val: 0.6788, Test: 0.6694\n",
      "Epoch: 018, Loss: 1.2495, Train: 0.7120, Val: 0.6971, Test: 0.6885\n",
      "Epoch: 019, Loss: 1.1331, Train: 0.7148, Val: 0.6963, Test: 0.7001\n",
      "Epoch: 020, Loss: 1.1263, Train: 0.7207, Val: 0.7095, Test: 0.7084\n",
      "Epoch: 021, Loss: 1.1082, Train: 0.7264, Val: 0.7178, Test: 0.7117\n",
      "Epoch: 022, Loss: 1.0761, Train: 0.7339, Val: 0.7212, Test: 0.7266\n",
      "Epoch: 023, Loss: 1.0247, Train: 0.7383, Val: 0.7178, Test: 0.7291\n",
      "Epoch: 024, Loss: 1.0270, Train: 0.7460, Val: 0.7286, Test: 0.7415\n",
      "Epoch: 025, Loss: 1.0089, Train: 0.7476, Val: 0.7378, Test: 0.7423\n",
      "Epoch: 026, Loss: 0.9930, Train: 0.7527, Val: 0.7378, Test: 0.7423\n",
      "Epoch: 027, Loss: 0.9733, Train: 0.7552, Val: 0.7427, Test: 0.7448\n",
      "Epoch: 028, Loss: 0.9391, Train: 0.7618, Val: 0.7510, Test: 0.7531\n",
      "Epoch: 029, Loss: 0.9430, Train: 0.7692, Val: 0.7552, Test: 0.7606\n",
      "Epoch: 030, Loss: 0.9515, Train: 0.7723, Val: 0.7618, Test: 0.7639\n",
      "Epoch: 031, Loss: 0.9206, Train: 0.7760, Val: 0.7668, Test: 0.7630\n",
      "Epoch: 032, Loss: 0.9350, Train: 0.7742, Val: 0.7660, Test: 0.7639\n",
      "Epoch: 033, Loss: 0.8950, Train: 0.7733, Val: 0.7635, Test: 0.7647\n",
      "Epoch: 034, Loss: 0.8738, Train: 0.7765, Val: 0.7643, Test: 0.7630\n",
      "Epoch: 035, Loss: 0.8760, Train: 0.7769, Val: 0.7627, Test: 0.7622\n",
      "Epoch: 036, Loss: 0.8505, Train: 0.7799, Val: 0.7685, Test: 0.7606\n",
      "Epoch: 037, Loss: 0.8374, Train: 0.7858, Val: 0.7759, Test: 0.7672\n",
      "Epoch: 038, Loss: 0.8588, Train: 0.7902, Val: 0.7793, Test: 0.7697\n",
      "Epoch: 039, Loss: 0.8289, Train: 0.7904, Val: 0.7809, Test: 0.7688\n",
      "Epoch: 040, Loss: 0.8351, Train: 0.7918, Val: 0.7759, Test: 0.7713\n",
      "Epoch: 041, Loss: 0.8332, Train: 0.7938, Val: 0.7759, Test: 0.7763\n",
      "Epoch: 042, Loss: 0.8257, Train: 0.7964, Val: 0.7793, Test: 0.7813\n",
      "Epoch: 043, Loss: 0.8025, Train: 0.7993, Val: 0.7801, Test: 0.7804\n",
      "Epoch: 044, Loss: 0.8029, Train: 0.8000, Val: 0.7826, Test: 0.7854\n",
      "Epoch: 045, Loss: 0.7878, Train: 0.8011, Val: 0.7876, Test: 0.7854\n",
      "Epoch: 046, Loss: 0.7954, Train: 0.8018, Val: 0.7851, Test: 0.7862\n",
      "Epoch: 047, Loss: 0.7898, Train: 0.8055, Val: 0.7892, Test: 0.7879\n",
      "Epoch: 048, Loss: 0.7603, Train: 0.8057, Val: 0.7909, Test: 0.7929\n",
      "Epoch: 049, Loss: 0.7775, Train: 0.8064, Val: 0.7959, Test: 0.7896\n",
      "Epoch: 050, Loss: 0.7509, Train: 0.8078, Val: 0.8000, Test: 0.7945\n",
      "Epoch: 051, Loss: 0.7583, Train: 0.8076, Val: 0.8008, Test: 0.7937\n",
      "Epoch: 052, Loss: 0.7439, Train: 0.8076, Val: 0.7992, Test: 0.7962\n",
      "Epoch: 053, Loss: 0.7506, Train: 0.8091, Val: 0.7975, Test: 0.7987\n",
      "Epoch: 054, Loss: 0.7358, Train: 0.8100, Val: 0.8025, Test: 0.7987\n",
      "Epoch: 055, Loss: 0.7094, Train: 0.8107, Val: 0.8017, Test: 0.7945\n",
      "Epoch: 056, Loss: 0.7099, Train: 0.8121, Val: 0.8058, Test: 0.7970\n",
      "Epoch: 057, Loss: 0.7286, Train: 0.8140, Val: 0.8066, Test: 0.7962\n",
      "Epoch: 058, Loss: 0.7242, Train: 0.8153, Val: 0.8058, Test: 0.7987\n",
      "Epoch: 059, Loss: 0.7033, Train: 0.8160, Val: 0.8058, Test: 0.8012\n",
      "Epoch: 060, Loss: 0.7271, Train: 0.8172, Val: 0.8058, Test: 0.7995\n",
      "Epoch: 061, Loss: 0.6988, Train: 0.8174, Val: 0.8041, Test: 0.7978\n",
      "Epoch: 062, Loss: 0.6834, Train: 0.8172, Val: 0.8017, Test: 0.7987\n",
      "Epoch: 063, Loss: 0.6818, Train: 0.8185, Val: 0.8000, Test: 0.7978\n",
      "Epoch: 064, Loss: 0.6888, Train: 0.8196, Val: 0.8058, Test: 0.7995\n",
      "Epoch: 065, Loss: 0.6707, Train: 0.8192, Val: 0.8083, Test: 0.8012\n",
      "Epoch: 066, Loss: 0.6767, Train: 0.8208, Val: 0.8083, Test: 0.8036\n",
      "Epoch: 067, Loss: 0.6705, Train: 0.8220, Val: 0.8116, Test: 0.8028\n",
      "Epoch: 068, Loss: 0.6613, Train: 0.8222, Val: 0.8158, Test: 0.8020\n",
      "Epoch: 069, Loss: 0.6548, Train: 0.8240, Val: 0.8174, Test: 0.8045\n",
      "Epoch: 070, Loss: 0.6607, Train: 0.8267, Val: 0.8191, Test: 0.8070\n",
      "Epoch: 071, Loss: 0.6551, Train: 0.8292, Val: 0.8191, Test: 0.8103\n",
      "Epoch: 072, Loss: 0.6551, Train: 0.8320, Val: 0.8166, Test: 0.8078\n",
      "Epoch: 073, Loss: 0.6288, Train: 0.8377, Val: 0.8166, Test: 0.8111\n",
      "Epoch: 074, Loss: 0.6676, Train: 0.8373, Val: 0.8174, Test: 0.8128\n",
      "Epoch: 075, Loss: 0.6270, Train: 0.8396, Val: 0.8232, Test: 0.8177\n",
      "Epoch: 076, Loss: 0.6405, Train: 0.8404, Val: 0.8224, Test: 0.8202\n",
      "Epoch: 077, Loss: 0.6376, Train: 0.8396, Val: 0.8249, Test: 0.8186\n",
      "Epoch: 078, Loss: 0.6535, Train: 0.8343, Val: 0.8224, Test: 0.8103\n",
      "Epoch: 079, Loss: 0.6351, Train: 0.8324, Val: 0.8191, Test: 0.8086\n",
      "Epoch: 080, Loss: 0.6188, Train: 0.8297, Val: 0.8166, Test: 0.8045\n",
      "Epoch: 081, Loss: 0.6254, Train: 0.8306, Val: 0.8158, Test: 0.8070\n",
      "Epoch: 082, Loss: 0.6170, Train: 0.8325, Val: 0.8207, Test: 0.8094\n",
      "Epoch: 083, Loss: 0.6440, Train: 0.8331, Val: 0.8266, Test: 0.8111\n",
      "Epoch: 084, Loss: 0.6155, Train: 0.8338, Val: 0.8282, Test: 0.8161\n",
      "Epoch: 085, Loss: 0.6226, Train: 0.8361, Val: 0.8315, Test: 0.8202\n",
      "Epoch: 086, Loss: 0.6287, Train: 0.8409, Val: 0.8332, Test: 0.8177\n",
      "Epoch: 087, Loss: 0.6224, Train: 0.8393, Val: 0.8307, Test: 0.8152\n",
      "Epoch: 088, Loss: 0.6044, Train: 0.8382, Val: 0.8249, Test: 0.8161\n",
      "Epoch: 089, Loss: 0.5971, Train: 0.8386, Val: 0.8249, Test: 0.8136\n",
      "Epoch: 090, Loss: 0.6106, Train: 0.8418, Val: 0.8274, Test: 0.8144\n",
      "Epoch: 091, Loss: 0.6255, Train: 0.8437, Val: 0.8315, Test: 0.8219\n",
      "Epoch: 092, Loss: 0.5938, Train: 0.8409, Val: 0.8332, Test: 0.8227\n",
      "Epoch: 093, Loss: 0.5930, Train: 0.8416, Val: 0.8332, Test: 0.8161\n",
      "Epoch: 094, Loss: 0.5881, Train: 0.8425, Val: 0.8274, Test: 0.8210\n",
      "Epoch: 095, Loss: 0.5883, Train: 0.8446, Val: 0.8266, Test: 0.8202\n",
      "Epoch: 096, Loss: 0.6055, Train: 0.8476, Val: 0.8282, Test: 0.8252\n",
      "Epoch: 097, Loss: 0.5890, Train: 0.8462, Val: 0.8324, Test: 0.8194\n",
      "Epoch: 098, Loss: 0.5805, Train: 0.8453, Val: 0.8324, Test: 0.8202\n",
      "Epoch: 099, Loss: 0.5837, Train: 0.8427, Val: 0.8365, Test: 0.8219\n",
      "Epoch: 100, Loss: 0.5652, Train: 0.8462, Val: 0.8382, Test: 0.8293\n",
      "Epoch: 101, Loss: 0.5793, Train: 0.8418, Val: 0.8307, Test: 0.8194\n",
      "Epoch: 102, Loss: 0.5697, Train: 0.8455, Val: 0.8324, Test: 0.8235\n",
      "Epoch: 103, Loss: 0.5738, Train: 0.8489, Val: 0.8365, Test: 0.8244\n",
      "Epoch: 104, Loss: 0.5710, Train: 0.8473, Val: 0.8357, Test: 0.8227\n",
      "Epoch: 105, Loss: 0.5875, Train: 0.8492, Val: 0.8315, Test: 0.8227\n",
      "Epoch: 106, Loss: 0.5629, Train: 0.8473, Val: 0.8282, Test: 0.8235\n",
      "Epoch: 107, Loss: 0.5660, Train: 0.8471, Val: 0.8257, Test: 0.8244\n",
      "Epoch: 108, Loss: 0.5525, Train: 0.8487, Val: 0.8257, Test: 0.8268\n",
      "Epoch: 109, Loss: 0.5549, Train: 0.8484, Val: 0.8307, Test: 0.8235\n",
      "Epoch: 110, Loss: 0.5555, Train: 0.8482, Val: 0.8299, Test: 0.8260\n",
      "Epoch: 111, Loss: 0.5598, Train: 0.8514, Val: 0.8332, Test: 0.8235\n",
      "Epoch: 112, Loss: 0.5465, Train: 0.8519, Val: 0.8357, Test: 0.8260\n",
      "Epoch: 113, Loss: 0.5533, Train: 0.8507, Val: 0.8373, Test: 0.8268\n",
      "Epoch: 114, Loss: 0.5424, Train: 0.8505, Val: 0.8373, Test: 0.8260\n",
      "Epoch: 115, Loss: 0.5482, Train: 0.8512, Val: 0.8349, Test: 0.8252\n",
      "Epoch: 116, Loss: 0.5305, Train: 0.8532, Val: 0.8382, Test: 0.8293\n",
      "Epoch: 117, Loss: 0.5423, Train: 0.8498, Val: 0.8365, Test: 0.8268\n",
      "Epoch: 118, Loss: 0.5394, Train: 0.8498, Val: 0.8390, Test: 0.8351\n",
      "Epoch: 119, Loss: 0.5540, Train: 0.8512, Val: 0.8390, Test: 0.8302\n",
      "Epoch: 120, Loss: 0.5378, Train: 0.8500, Val: 0.8365, Test: 0.8335\n",
      "Epoch: 121, Loss: 0.5459, Train: 0.8512, Val: 0.8349, Test: 0.8343\n",
      "Epoch: 122, Loss: 0.5385, Train: 0.8539, Val: 0.8398, Test: 0.8360\n",
      "Epoch: 123, Loss: 0.5307, Train: 0.8565, Val: 0.8407, Test: 0.8351\n",
      "Epoch: 124, Loss: 0.5315, Train: 0.8539, Val: 0.8390, Test: 0.8302\n",
      "Epoch: 125, Loss: 0.5272, Train: 0.8537, Val: 0.8398, Test: 0.8293\n",
      "Epoch: 126, Loss: 0.5405, Train: 0.8546, Val: 0.8415, Test: 0.8302\n",
      "Epoch: 127, Loss: 0.5387, Train: 0.8567, Val: 0.8432, Test: 0.8343\n",
      "Epoch: 128, Loss: 0.5300, Train: 0.8571, Val: 0.8423, Test: 0.8326\n",
      "Epoch: 129, Loss: 0.5254, Train: 0.8592, Val: 0.8373, Test: 0.8343\n",
      "Epoch: 130, Loss: 0.5215, Train: 0.8599, Val: 0.8382, Test: 0.8360\n",
      "Epoch: 131, Loss: 0.5216, Train: 0.8606, Val: 0.8415, Test: 0.8368\n",
      "Epoch: 132, Loss: 0.5289, Train: 0.8612, Val: 0.8423, Test: 0.8376\n",
      "Epoch: 133, Loss: 0.5278, Train: 0.8606, Val: 0.8456, Test: 0.8368\n",
      "Epoch: 134, Loss: 0.5112, Train: 0.8578, Val: 0.8423, Test: 0.8360\n",
      "Epoch: 135, Loss: 0.5245, Train: 0.8576, Val: 0.8407, Test: 0.8360\n",
      "Epoch: 136, Loss: 0.5061, Train: 0.8565, Val: 0.8415, Test: 0.8368\n",
      "Epoch: 137, Loss: 0.5207, Train: 0.8556, Val: 0.8423, Test: 0.8360\n",
      "Epoch: 138, Loss: 0.5064, Train: 0.8556, Val: 0.8407, Test: 0.8376\n",
      "Epoch: 139, Loss: 0.5140, Train: 0.8553, Val: 0.8407, Test: 0.8393\n",
      "Epoch: 140, Loss: 0.5407, Train: 0.8578, Val: 0.8382, Test: 0.8393\n",
      "Epoch: 141, Loss: 0.5162, Train: 0.8564, Val: 0.8365, Test: 0.8376\n",
      "Epoch: 142, Loss: 0.5166, Train: 0.8553, Val: 0.8340, Test: 0.8384\n",
      "Epoch: 143, Loss: 0.5096, Train: 0.8581, Val: 0.8407, Test: 0.8426\n",
      "Epoch: 144, Loss: 0.5150, Train: 0.8601, Val: 0.8473, Test: 0.8434\n",
      "Epoch: 145, Loss: 0.5044, Train: 0.8597, Val: 0.8440, Test: 0.8368\n",
      "Epoch: 146, Loss: 0.5057, Train: 0.8597, Val: 0.8440, Test: 0.8368\n",
      "Epoch: 147, Loss: 0.5382, Train: 0.8466, Val: 0.8349, Test: 0.8302\n",
      "Epoch: 148, Loss: 0.5979, Train: 0.8475, Val: 0.8415, Test: 0.8285\n",
      "Epoch: 149, Loss: 0.5493, Train: 0.8548, Val: 0.8415, Test: 0.8368\n",
      "Epoch: 150, Loss: 0.5332, Train: 0.8564, Val: 0.8349, Test: 0.8343\n",
      "Epoch: 151, Loss: 0.5393, Train: 0.8533, Val: 0.8315, Test: 0.8384\n",
      "Epoch: 152, Loss: 0.5483, Train: 0.8619, Val: 0.8465, Test: 0.8476\n",
      "Epoch: 153, Loss: 0.5544, Train: 0.8556, Val: 0.8357, Test: 0.8442\n",
      "Epoch: 154, Loss: 0.5302, Train: 0.8608, Val: 0.8415, Test: 0.8426\n",
      "Epoch: 155, Loss: 0.5381, Train: 0.8539, Val: 0.8332, Test: 0.8368\n",
      "Epoch: 156, Loss: 0.5348, Train: 0.8528, Val: 0.8290, Test: 0.8302\n",
      "Epoch: 157, Loss: 0.5196, Train: 0.8548, Val: 0.8307, Test: 0.8285\n",
      "Epoch: 158, Loss: 0.5124, Train: 0.8599, Val: 0.8448, Test: 0.8393\n",
      "Epoch: 159, Loss: 0.5218, Train: 0.8578, Val: 0.8432, Test: 0.8409\n",
      "Epoch: 160, Loss: 0.5151, Train: 0.8524, Val: 0.8382, Test: 0.8360\n",
      "Epoch: 161, Loss: 0.5265, Train: 0.8528, Val: 0.8390, Test: 0.8368\n",
      "Epoch: 162, Loss: 0.5229, Train: 0.8564, Val: 0.8390, Test: 0.8343\n",
      "Epoch: 163, Loss: 0.5023, Train: 0.8588, Val: 0.8465, Test: 0.8343\n",
      "Epoch: 164, Loss: 0.5104, Train: 0.8626, Val: 0.8456, Test: 0.8376\n",
      "Epoch: 165, Loss: 0.4984, Train: 0.8642, Val: 0.8440, Test: 0.8434\n",
      "Epoch: 166, Loss: 0.4995, Train: 0.8617, Val: 0.8407, Test: 0.8426\n",
      "Epoch: 167, Loss: 0.4952, Train: 0.8644, Val: 0.8465, Test: 0.8476\n",
      "Epoch: 168, Loss: 0.4912, Train: 0.8667, Val: 0.8481, Test: 0.8550\n",
      "Epoch: 169, Loss: 0.4934, Train: 0.8645, Val: 0.8456, Test: 0.8534\n",
      "Epoch: 170, Loss: 0.4897, Train: 0.8631, Val: 0.8481, Test: 0.8492\n",
      "Epoch: 171, Loss: 0.4980, Train: 0.8644, Val: 0.8531, Test: 0.8492\n",
      "Epoch: 172, Loss: 0.4851, Train: 0.8649, Val: 0.8539, Test: 0.8467\n",
      "Epoch: 173, Loss: 0.4973, Train: 0.8624, Val: 0.8523, Test: 0.8459\n",
      "Epoch: 174, Loss: 0.5010, Train: 0.8635, Val: 0.8548, Test: 0.8509\n",
      "Epoch: 175, Loss: 0.4968, Train: 0.8683, Val: 0.8539, Test: 0.8583\n",
      "Epoch: 176, Loss: 0.4733, Train: 0.8679, Val: 0.8515, Test: 0.8583\n",
      "Epoch: 177, Loss: 0.4766, Train: 0.8676, Val: 0.8539, Test: 0.8509\n",
      "Epoch: 178, Loss: 0.4945, Train: 0.8647, Val: 0.8473, Test: 0.8467\n",
      "Epoch: 179, Loss: 0.5008, Train: 0.8640, Val: 0.8415, Test: 0.8500\n",
      "Epoch: 180, Loss: 0.4766, Train: 0.8620, Val: 0.8390, Test: 0.8434\n",
      "Epoch: 181, Loss: 0.4812, Train: 0.8585, Val: 0.8349, Test: 0.8351\n",
      "Epoch: 182, Loss: 0.5054, Train: 0.8640, Val: 0.8448, Test: 0.8418\n",
      "Epoch: 183, Loss: 0.4758, Train: 0.8656, Val: 0.8523, Test: 0.8418\n",
      "Epoch: 184, Loss: 0.4945, Train: 0.8636, Val: 0.8423, Test: 0.8426\n",
      "Epoch: 185, Loss: 0.4989, Train: 0.8644, Val: 0.8490, Test: 0.8500\n",
      "Epoch: 186, Loss: 0.4977, Train: 0.8665, Val: 0.8456, Test: 0.8534\n",
      "Epoch: 187, Loss: 0.4838, Train: 0.8699, Val: 0.8448, Test: 0.8525\n",
      "Epoch: 188, Loss: 0.4895, Train: 0.8677, Val: 0.8556, Test: 0.8476\n",
      "Epoch: 189, Loss: 0.4849, Train: 0.8695, Val: 0.8515, Test: 0.8467\n",
      "Epoch: 190, Loss: 0.4721, Train: 0.8670, Val: 0.8498, Test: 0.8484\n",
      "Epoch: 191, Loss: 0.4691, Train: 0.8661, Val: 0.8548, Test: 0.8467\n",
      "Epoch: 192, Loss: 0.4712, Train: 0.8645, Val: 0.8531, Test: 0.8517\n",
      "Epoch: 193, Loss: 0.4801, Train: 0.8636, Val: 0.8515, Test: 0.8401\n",
      "Epoch: 194, Loss: 0.4663, Train: 0.8708, Val: 0.8614, Test: 0.8592\n",
      "Epoch: 195, Loss: 0.4621, Train: 0.8734, Val: 0.8564, Test: 0.8567\n",
      "Epoch: 196, Loss: 0.4669, Train: 0.8725, Val: 0.8539, Test: 0.8575\n",
      "Epoch: 197, Loss: 0.4729, Train: 0.8716, Val: 0.8523, Test: 0.8550\n",
      "Epoch: 198, Loss: 0.4608, Train: 0.8729, Val: 0.8606, Test: 0.8600\n",
      "Epoch: 199, Loss: 0.4618, Train: 0.8738, Val: 0.8598, Test: 0.8592\n",
      "Epoch: 200, Loss: 0.4686, Train: 0.8709, Val: 0.8515, Test: 0.8500\n",
      "\n",
      "Final Results:\n",
      "Best Model - Train: 0.8708, Val: 0.8614, Test: 0.8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13925/4083622090.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_training(\n",
    "        root='./processed_data',\n",
    "        path=['nepali_music_metadata.csv', 'nepali_features.hdf5'],\n",
    "        num_epochs=200\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def2ec3-8b4c-4091-be00-e687766a0dbb",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e41faf13-e7a1-4919-98ce-7f523f5def34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f35bdd2-e2b0-4c6c-8e3e-85d08adbed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTester:\n",
    "    def __init__(self, model_path, train_stats, k_neigh=5, device='cuda'):\n",
    "        self.device = device\n",
    "        self.k_neigh = k_neigh\n",
    "        self.model = torch.load(model_path).to(device).eval()\n",
    "        self.train_mean, self.train_std = train_stats\n",
    "        \n",
    "        # YamNet initialization (match your training setup)\n",
    "        self.yamnet = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "\n",
    "    def process_audio(self, audio_dir):\n",
    "        \"\"\"Process raw audio files into graph format\"\"\"\n",
    "        # Feature extraction\n",
    "        features, paths = self._extract_features(audio_dir)\n",
    "        \n",
    "        # Normalize using training statistics\n",
    "        features = (features - self.train_mean) / self.train_std\n",
    "        \n",
    "        # Create test graph\n",
    "        edge_index = knn_graph(torch.tensor(features), k=self.k_neigh, loop=False)\n",
    "        \n",
    "        return Data(\n",
    "            x=torch.tensor(features, dtype=torch.float32).to(self.device),\n",
    "            edge_index=edge_index.to(self.device),\n",
    "            paths=paths\n",
    "        )\n",
    "\n",
    "    def _extract_features(self, audio_dir):\n",
    "        \"\"\"Extract YamNet features matching training setup\"\"\"\n",
    "        features = []\n",
    "        paths = []\n",
    "        \n",
    "        for audio_path in tqdm(list(Path(audio_dir).glob('**/*.wav'))):\n",
    "            try:\n",
    "                # Match your training feature extraction\n",
    "                audio = librosa.load(audio_path, sr=16000, mono=True)[0]\n",
    "                _, embeddings, _ = self.yamnet(audio)\n",
    "                features.append(np.mean(embeddions.numpy(), axis=0))\n",
    "                paths.append(str(audio_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {audio_path}: {str(e)}\")\n",
    "        \n",
    "        return np.array(features), paths\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        \"\"\"Run inference on processed test graph\"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(test_data)\n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'path': test_data.paths,\n",
    "            'prediction': preds,\n",
    "            'confidence': probs.max(axis=1),\n",
    "            'probabilities': list(probs)\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdd041-0dea-46e0-a672-2b0f29e91e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2682897-720b-46e6-bfd6-780f2e270f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
